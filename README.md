# ğŸ“š Marcus Local MCP Server

**Context for AI to retrieve** - A Model Context Protocol (MCP) server that provides semantic search over indexed documentation sources for AI assistants.

![Next.js](https://img.shields.io/badge/Next.js-15.5.4-black?logo=next.js)
![Python](https://img.shields.io/badge/Python-3.13-blue?logo=python)
![MCP](https://img.shields.io/badge/MCP-1.0-green)
![ChromaDB](https://img.shields.io/badge/ChromaDB-latest-orange)

## ğŸ¯ What Is This?

This is a **local MCP (Model Context Protocol) server** that enables AI assistants like Claude Desktop, ChatGPT, and others to search through documentation you've indexed. It works by:

1. **Crawling** documentation sites using Crawl4AI
2. **Indexing** content with OpenAI embeddings into ChromaDB (vector database)
3. **Exposing** an MCP server that AI assistants can query semantically
4. **Providing** a web UI for managing sources and testing searches

Think of it as giving your AI assistant access to searchable, indexed documentation from any website you choose.

## ğŸš€ Key Features

### For AI Assistants
- ğŸ¤– **MCP Protocol Support** - Standard protocol for AI assistant integration
- ğŸ” **Semantic Search** - Natural language queries across indexed docs
- ğŸ“š **Multi-Source** - Index and search multiple documentation sites
- âš¡ **Fast Retrieval** - Vector-based search with ChromaDB
- ğŸ¯ **Source Filtering** - Search specific documentation sources

### For Users (Web UI)
- ğŸŒ **Add Documentation** - Crawl and index any documentation site
- ğŸ—‘ï¸ **Manage Sources** - Delete sources, view pages and chunks
- ğŸ” **Test Searches** - Try semantic searches in the browser
- ğŸ“Š **View Statistics** - See pages, chunks, and word counts
- ğŸ¨ **Modern Interface** - Beautiful UI built with Next.js and shadcn/ui

## ğŸ“‹ How It Works

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AI Assistant   â”‚ (Claude, ChatGPT, etc.)
â”‚  (via MCP)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   MCP Server    â”‚ (Python - stdio)
â”‚   main.py       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ChromaDB      â”‚â—„â”€â”€â”€â”€â”€â”¤   OpenAI     â”‚
â”‚  (Vector Store) â”‚      â”‚  Embeddings  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Indexed Docs   â”‚
â”‚  â€¢ Moca Network â”‚
â”‚  â€¢ Solana       â”‚
â”‚  â€¢ Your Docs    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Separate Web UI** for management:
```
Next.js UI â†’ Python Scripts â†’ Crawl4AI â†’ Index â†’ ChromaDB
```

## ğŸ› ï¸ Installation

### 1. Clone and Setup
```bash
git clone <your-repo>
cd crawl4ai_test

# Install Node.js dependencies
npm install

# Setup Python virtual environment
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install Python dependencies
pip install -r mcp-docs-server/requirements.txt
```

### 2. Configure Environment
Create a `.env` file in `mcp-docs-server/`:
```bash
OPENAI_API_KEY=your_openai_api_key_here
EMBEDDING_MODEL=text-embedding-3-small
DEFAULT_RESULTS=5
```

### 3. Initial Setup
```bash
# Install Crawl4AI
pip install -U crawl4ai

# Run Crawl4AI setup
crawl4ai-setup

# Verify installation
crawl4ai-doctor
```

## ğŸš€ Usage

### Running the Web UI

```bash
# Terminal 1: Start Next.js dev server
npm run dev

# Open browser
open http://localhost:3000
```

The web UI allows you to:
- âœ… Add new documentation sources
- âœ… View indexed sources and their pages
- âœ… Test semantic searches
- âœ… Delete sources
- âœ… View indexing statistics

### Running the MCP Server

The MCP server runs via stdio for AI assistant integration:

```bash
cd mcp-docs-server
source ../venv/bin/activate
python server/main.py
```

### Connecting to Claude Desktop

Add to your Claude Desktop config (`~/Library/Application Support/Claude/claude_desktop_config.json`):

```json
{
  "mcpServers": {
    "marcus-docs": {
      "command": "/path/to/your/venv/bin/python",
      "args": [
        "/path/to/crawl4ai_test/mcp-docs-server/server/main.py"
      ]
    }
  }
}
```

Now Claude can search your indexed documentation!

## ğŸ“– Adding Documentation

### Via Web UI (Easiest)

1. Go to http://localhost:3000
2. Click "Add New Docs"
3. Enter:
   - **URL**: `https://docs.example.com`
   - **Source Name**: `Example Docs`
   - **Max Pages**: `50` (or unlimited)
4. Click "Start Indexing"
5. Wait for crawling and indexing to complete

### Via Command Line

```bash
cd mcp-docs-server
source ../venv/bin/activate

# Crawl documentation
python scripts/crawler.py https://docs.example.com "Example Docs" 50

# Index the crawled content
python scripts/indexer.py "Example Docs"
```

## ğŸ” Using the Search

### From Web UI
1. Enter a natural language query: "How do I initialize the SDK?"
2. Select a source (or search all)
3. Click "Search Documentation"

### From AI Assistant
Once connected via MCP, your AI assistant can search automatically:

```
User: Can you search my docs for information about API authentication?

AI: [Uses MCP to search your indexed docs]
    Here's what I found in your documentation...
```

## ğŸ“ Project Structure

```
crawl4ai_test/
â”œâ”€â”€ pages/                      # Next.js pages
â”‚   â”œâ”€â”€ index.js               # Main UI (270 lines - refactored!)
â”‚   â”œâ”€â”€ add.js                 # Add documentation page
â”‚   â””â”€â”€ api/                   # Next.js API routes
â”‚       â”œâ”€â”€ mcp-search.js      # Search endpoint
â”‚       â”œâ”€â”€ mcp-info.js        # Index info endpoint
â”‚       â”œâ”€â”€ mcp-delete-source.js
â”‚       â”œâ”€â”€ add-docs-crawl.js  # Crawl endpoint
â”‚       â””â”€â”€ add-docs-index.js  # Index endpoint
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ ui/                    # shadcn/ui components
â”‚   â””â”€â”€ home/                  # Modular page components
â”‚       â”œâ”€â”€ Header.jsx
â”‚       â”œâ”€â”€ Footer.jsx
â”‚       â”œâ”€â”€ SearchCard.jsx
â”‚       â”œâ”€â”€ SearchResults.jsx
â”‚       â”œâ”€â”€ IndexInfoCard.jsx
â”‚       â”œâ”€â”€ SourceItem.jsx
â”‚       â”œâ”€â”€ PageItem.jsx
â”‚       â”œâ”€â”€ ErrorDisplay.jsx
â”‚       â””â”€â”€ DeleteConfirmDialog.jsx
â”œâ”€â”€ mcp-docs-server/           # MCP Server & Scripts
â”‚   â”œâ”€â”€ server/
â”‚   â”‚   â””â”€â”€ main.py           # MCP server (stdio)
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â”œâ”€â”€ crawler.py        # Crawl4AI crawler
â”‚   â”‚   â”œâ”€â”€ indexer.py        # Index to ChromaDB
â”‚   â”‚   â”œâ”€â”€ search.py         # Search interface
â”‚   â”‚   â””â”€â”€ delete_source.py  # Delete sources
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ chroma_db/        # Vector database
â”‚   â”‚   â”œâ”€â”€ chunks/           # Metadata
â”‚   â”‚   â””â”€â”€ raw/              # Crawled JSON
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ venv/                      # Python virtual environment
â””â”€â”€ README.md                  # This file
```

## ğŸ¨ Tech Stack

### Frontend (Web UI)
- **Framework**: Next.js 15.5.4 (Pages Router)
- **UI Components**: shadcn/ui
- **Styling**: Tailwind CSS 4
- **Icons**: Lucide React

### Backend (MCP Server)
- **Protocol**: Model Context Protocol (MCP)
- **Language**: Python 3.13
- **Crawler**: Crawl4AI
- **Vector DB**: ChromaDB
- **Embeddings**: OpenAI (text-embedding-3-small)
- **Server**: stdio-based MCP server

## ğŸ”§ Configuration

### Environment Variables

```bash
# mcp-docs-server/.env
OPENAI_API_KEY=sk-...              # Required: OpenAI API key
EMBEDDING_MODEL=text-embedding-3-small  # Embedding model
DEFAULT_RESULTS=5                   # Default search results
```

### Indexing Parameters

- **Chunk Size**: 800 tokens (default)
- **Chunk Overlap**: 100 tokens
- **Max Pages**: Configurable per source
- **Embedding Model**: text-embedding-3-small

## ğŸ“Š Example Use Cases

### 1. Developer Documentation
Index your company's internal documentation and give AI assistants instant access:
```bash
# Add your company docs
python scripts/crawler.py https://docs.yourcompany.com "Company Docs"
python scripts/indexer.py "Company Docs"
```

### 2. Multiple Projects
Index documentation for all your projects:
- Solana documentation
- Moca Network documentation  
- Your API documentation
- Third-party library docs

### 3. Research & Learning
Index educational resources and let AI help you learn:
- Course materials
- Technical papers
- Tutorial sites

## ğŸ› Troubleshooting

### ChromaDB Issues
```bash
# Reset the database
rm -rf mcp-docs-server/data/chroma_db/*
# Re-index your sources
```

### OpenAI API Errors
- Check your API key in `.env`
- Verify you have credits available
- Check rate limits

### Crawl4AI Issues
```bash
# Re-run setup
crawl4ai-setup

# Check installation
crawl4ai-doctor
```

### Next.js Issues
```bash
# Clear cache
rm -rf .next
npm run dev
```

## ğŸ”’ Privacy & Security

- âœ… **Runs 100% locally** - Your data stays on your machine
- âœ… **No external storage** - ChromaDB is local
- âœ… **API keys protected** - Environment variables only
- âš ï¸ **OpenAI API** - Only embeddings are sent (not full docs)

## ğŸ“ˆ Performance

- **Search Speed**: < 100ms for typical queries
- **Indexing**: ~1-2 seconds per page
- **Storage**: ~1-2MB per 100 pages indexed
- **Memory**: ChromaDB uses ~200MB RAM

## ğŸ¤ About the Technologies

### Model Context Protocol (MCP)
MCP is Anthropic's open protocol for connecting AI assistants to external data sources. This server implements MCP to provide documentation search.

- **GitHub**: [modelcontextprotocol/servers](https://github.com/modelcontextprotocol/servers)
- **Docs**: [modelcontextprotocol.io](https://modelcontextprotocol.io)

### Crawl4AI
Open-source LLM-friendly web crawler that turns websites into clean, structured data.

- **GitHub**: [unclecode/crawl4ai](https://github.com/unclecode/crawl4ai)
- **Stars**: 54.3k+
- **License**: Apache 2.0

## ğŸ¯ Roadmap

### Current Features âœ…
- Multi-source documentation indexing
- Semantic search via MCP
- Web UI for management
- Source deletion and viewing
- Page and chunk inspection

### Planned Features ğŸš€
- [ ] Incremental updates (re-index changed pages)
- [ ] Custom chunking strategies
- [ ] Multiple embedding models
- [ ] Export/import indices
- [ ] Search analytics
- [ ] Scheduled re-indexing
- [ ] Authentication for web UI

## ğŸ“ License

This project is open source. Crawl4AI is licensed under Apache 2.0.

## ğŸ™ Credits

- **Crawl4AI** by [@unclecode](https://github.com/unclecode)
- **Model Context Protocol** by [Anthropic](https://anthropic.com)
- **shadcn/ui** by [@shadcn](https://github.com/shadcn)
- **ChromaDB** by [Chroma](https://www.trychroma.com)

---

**Status**: âœ… Fully Operational | ğŸ¤– MCP Server Ready | ğŸ” Search Enabled

Built with â¤ï¸ to make documentation accessible to AI assistants
