# ğŸ“š Marcus Local MCP Server

A Model Context Protocol (MCP) server that indexes documentation sites and local code repositories for semantic search by AI assistants.

![Next.js](https://img.shields.io/badge/Next.js-15.5.4-black?logo=next.js)
![Python](https://img.shields.io/badge/Python-3.13-blue?logo=python)
![MCP](https://img.shields.io/badge/MCP-1.0-green)
![ChromaDB](https://img.shields.io/badge/ChromaDB-latest-orange)

## ğŸ¯ What Is This?

This is a **local MCP server** that enables AI assistants (Cursor, Claude Desktop, ChatGPT) to semantically search through:
- **Documentation websites** - Crawled and indexed from any docs site
- **Local code repositories** - All text files from your projects

It uses OpenAI embeddings to create a vector database (ChromaDB) that AI assistants can query through the Model Context Protocol.

**Think of it as:** Giving your AI assistant instant access to searchable documentation and your entire codebase.

## ğŸ“‹ How It Works

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AI Assistant   â”‚ (Cursor, Claude, ChatGPT, etc.)
â”‚  (via MCP)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   MCP Server    â”‚ (Python - stdio)
â”‚   main.py       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ChromaDB      â”‚â—„â”€â”€â”€â”€â”€â”¤   OpenAI     â”‚
â”‚  (Vector Store) â”‚      â”‚  Embeddings  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Indexed Sources    â”‚
â”‚  â€¢ Documentation     â”‚
â”‚    - Moca Network    â”‚
â”‚    - Your Docs       â”‚
â”‚  â€¢ Repositories      â”‚
â”‚    - Your Codebase   â”‚
â”‚    - Local Projects  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**The Flow:**
1. **Index** - Crawl docs OR read local repo files
2. **Chunk** - Split content into 800-token chunks
3. **Embed** - Create OpenAI embeddings (batched for speed)
4. **Store** - Save in ChromaDB vector database
5. **Search** - AI assistant queries via MCP protocol
6. **Retrieve** - Return relevant chunks from docs/code

## ğŸš€ How to Run It

### 1. Setup

```bash
# Clone repository
git clone <your-repo>
cd crawl4ai_test

# Install Node.js dependencies
npm install

# Setup Python virtual environment
python3 -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install Python dependencies
pip install -r mcp-docs-server/requirements.txt

# Install Crawl4AI
pip install -U crawl4ai
crawl4ai-setup
```

### 2. Configure

Create `.env` file in `mcp-docs-server/`:
```bash
OPENAI_API_KEY=your_openai_api_key_here
EMBEDDING_MODEL=text-embedding-3-small
DEFAULT_RESULTS=5
```

### 3. Run the Web UI

```bash
# Start Next.js server
npm run dev

# Open browser
open http://localhost:3030
```

### 4. Connect to Cursor/Claude

Add to your AI assistant config:

**For Cursor** (`~/.cursor/mcp.json` or project config):
```json
{
  "mcpServers": {
    "marcus-mcp-server": {
      "command": "/path/to/your/venv/bin/python3",
      "args": ["/path/to/crawl4ai_test/mcp-docs-server/server/main.py"]
    }
  }
}
```

**For Claude Desktop** (`~/Library/Application Support/Claude/claude_desktop_config.json`):
```json
{
  "mcpServers": {
    "marcus-docs": {
      "command": "/path/to/your/venv/bin/python",
      "args": ["/path/to/crawl4ai_test/mcp-docs-server/server/main.py"]
    }
  }
}
```

## ğŸ“– How to Use It

### Adding Documentation

**Via Web UI:**
1. Go to http://localhost:3030
2. Click **"Add New Docs"**
3. Enter:
   - **URL**: `https://docs.example.com`
   - **Source Name**: `Example Docs`
   - **Max Pages**: `50` (or unlimited)
4. Click **"Start Indexing"**
5. Wait for completion

**Via Command Line:**
```bash
cd mcp-docs-server
source ../venv/bin/activate

python scripts/crawler.py https://docs.example.com "Example Docs" 50
python scripts/indexer_multi.py "Example Docs"
```

### Adding Repositories

**Via Web UI:**
1. Go to http://localhost:3030
2. Click **"Add Repository"**
3. Enter:
   - **Repository Path**: Drag-and-drop folder OR paste path
   - **Source Name**: Auto-generated from folder name
4. Click **"Start Indexing"**
5. Watch live progress

**What gets indexed:**
- âœ… All text files (`.js`, `.py`, `.md`, `.tsx`, `.json`, `.css`, etc.)
- âœ… Auto-skips: `node_modules`, `.git`, `venv`, `build`, `.next`, etc.
- âœ… Batched embeddings (50-100x faster)

**Via Command Line:**
```bash
cd mcp-docs-server
source ../venv/bin/activate

python scripts/repo_indexer.py "/path/to/your/repo" "My Project"
```

### Searching

**From Web UI:**
1. Enter query: `"How do I initialize the SDK?"`
2. Select source (Docs, Repos, or All)
3. Click **"Search Documentation"**
4. View results

**From AI Assistant:**

Search all sources:
```
@marcus-mcp-server search for "authentication flow"
```

Filter by specific source:
```
@marcus-mcp-server search for "BorrowInterface component" 
with source="Credo Protocol"
```

Example usage in Cursor:
```
User: Using my marcus-mcp-server, show me how authentication 
      is implemented in the Credo Protocol repository

AI: [Searches indexed repository and returns relevant code chunks]
```

**Pro Tip:** Always filter by source name to get focused results and save context tokens.

### Managing Sources

**View Sources:**
- See all indexed docs and repos on the main page
- Filter by "Docs" or "Repos" tabs
- Expand to see individual pages/files

**Delete Sources:**
- Click trash icon next to any source
- Confirm deletion
- Source and all chunks are removed

## ğŸ“ Project Structure

```
crawl4ai_test/
â”œâ”€â”€ pages/                        # Next.js UI
â”‚   â”œâ”€â”€ index.js                 # Main page (search + sources)
â”‚   â”œâ”€â”€ add.js                   # Add documentation
â”‚   â”œâ”€â”€ add-repo.js              # Add repository
â”‚   â””â”€â”€ api/                     # API routes
â”‚       â”œâ”€â”€ mcp-search.js        # Search endpoint
â”‚       â”œâ”€â”€ mcp-info.js          # Get index info
â”‚       â”œâ”€â”€ add-docs-crawl.js    # Crawl docs
â”‚       â”œâ”€â”€ add-docs-index.js    # Index docs
â”‚       â”œâ”€â”€ add-repo-index.js    # Index repository
â”‚       â””â”€â”€ mcp-delete-source.js # Delete source
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ ui/                      # shadcn/ui components
â”‚   â””â”€â”€ home/                    # Page components
â”œâ”€â”€ mcp-docs-server/             # MCP Server
â”‚   â”œâ”€â”€ server/
â”‚   â”‚   â””â”€â”€ main.py             # MCP server (stdio)
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â”œâ”€â”€ crawler.py          # Crawl docs with Crawl4AI
â”‚   â”‚   â”œâ”€â”€ indexer_multi.py    # Index docs
â”‚   â”‚   â”œâ”€â”€ repo_indexer.py     # Index repositories
â”‚   â”‚   â”œâ”€â”€ get_source_pages.py # Get pages/files
â”‚   â”‚   â”œâ”€â”€ search.py           # Search
â”‚   â”‚   â””â”€â”€ delete_source.py    # Delete sources
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ chroma_db/          # Vector database
â”‚   â”‚   â”œâ”€â”€ chunks/             # Metadata
â”‚   â”‚   â””â”€â”€ raw/                # Crawled JSON
â”‚   â””â”€â”€ requirements.txt
â””â”€â”€ venv/                        # Python environment
```

## ğŸ¨ Built With

- **Frontend**: Next.js 15 + shadcn/ui + Tailwind CSS
- **Backend**: Python 3.13 + MCP Protocol
- **Crawler**: Crawl4AI
- **Vector DB**: ChromaDB
- **Embeddings**: OpenAI (text-embedding-3-small)

---

**Status**: âœ… Fully Operational | ğŸ¤– MCP Ready | ğŸ” Search Enabled
